{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: MapReduce implementation\n",
    "\n",
    "### Introduction\n",
    "In this project MapReduce implementation will be built.\n",
    "\n",
    "Counting words occurences within Wikipedia pages (999 pages)\n",
    "\n",
    "**Used modules:** os; math; multiprocessing; functools; csv; pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "Bay_of_ConcepciC3B3n.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "urls_path = \"./wiki/\"\n",
    "file_names = os.listdir(urls_path)\n",
    "\n",
    "print(len(file_names))\n",
    "\n",
    "print(file_names[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create needed functions:\n",
    "1. Split data into chunks\n",
    "2. MapReduce itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import functools\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def make_chunks(data, num_chunks):\n",
    "    len_data = len(data)\n",
    "    chunk_size = math.ceil(len_data / num_chunks)\n",
    "    chunks = [data[i : chunk_size + i] for i in range(0, len_data, chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def map_reduce(data, num_processes, mapper, reducer):\n",
    "    chunks = make_chunks(data, num_processes)\n",
    "    with Pool(num_processes) as pool:\n",
    "        chunk_results = pool.map(mapper, chunks)\n",
    "    return functools.reduce(reducer, chunk_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mapper and reducer functions to count total lines in all files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_count_lines(chunk):\n",
    "    num_lines = 0\n",
    "    for i in range(0, len(chunk)):\n",
    "        with open(os.path.join(urls_path, chunk[i]), encoding=\"UTF-8\") as file:\n",
    "            num_lines += len(file.readlines())\n",
    "    return num_lines\n",
    "\n",
    "def reduce_count_lines(n_lines1, n_lines2):\n",
    "    return n_lines1 + n_lines2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499797\n"
     ]
    }
   ],
   "source": [
    "n_lines = map_reduce(file_names, 4, map_count_lines, reduce_count_lines)\n",
    "print(n_lines)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a mapper, which finds all *target_word* occurences in html page. Writing page name, *target_word* line index and index of occurence into dictionary.\n",
    "### Also implementing a reducer for dictionaries and a function calling map_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 422), (45, 628), (45, 650), (58, 447), (58, 692), (60, 18), (62, 568), (62, 590), (105, 40), (105, 748), (105, 789), (105, 814), (188, 1039), (188, 1088), (188, 1132), (205, 125)]\n"
     ]
    }
   ],
   "source": [
    "target_word = \"data\"\n",
    "\n",
    "def map_word_lines(file_paths):\n",
    "    locations = {}\n",
    "    \n",
    "    for i in range(0, len(file_paths)):\n",
    "        fn = file_paths[i]\n",
    "        \n",
    "        with open(fn) as file:\n",
    "            lines = file.readlines()\n",
    "        \n",
    "            for j, line in enumerate(lines):\n",
    "                line = line.lower()\n",
    "                iof = 0\n",
    "                while line.find(target_word, iof) != -1:\n",
    "                    iof = line.find(target_word, iof)\n",
    "                    if fn not in locations:\n",
    "                        locations[fn] = [(j, iof)]\n",
    "                    else:\n",
    "                        locations[fn].append((j, iof))\n",
    "                    iof += 1\n",
    "    return locations\n",
    "\n",
    "\n",
    "def reduce_word_lines(loc1, loc2):\n",
    "    merged = {}\n",
    "    merged.update(loc1)\n",
    "    merged.update(loc2)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def map_reduce_grep(path, num_processes):\n",
    "    file_paths = [os.path.join(path, fn) for fn in os.listdir(path)]\n",
    "    return map_reduce(file_paths, num_processes, map_word_lines, reduce_word_lines)\n",
    "    \n",
    "\n",
    "word_locations = map_reduce_grep(urls_path, 4)\n",
    "import json\n",
    "print(word_locations[\"./wiki/Bay_of_ConcepciC3B3n.html\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing gathered data into csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Line</th>\n",
       "      <th>Index</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./wiki/Bay_of_ConcepciC3B3n.html</td>\n",
       "      <td>6</td>\n",
       "      <td>422</td>\n",
       "      <td>egories\":[\"Coordinates on Wikidata\",\"All stub ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./wiki/Bay_of_ConcepciC3B3n.html</td>\n",
       "      <td>45</td>\n",
       "      <td>628</td>\n",
       "      <td>78-sj18-04-quiriquina.jpg 2x\" data-file-width=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./wiki/Bay_of_ConcepciC3B3n.html</td>\n",
       "      <td>45</td>\n",
       "      <td>650</td>\n",
       "      <td>jpg 2x\" data-file-width=\"960\" data-file-height...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./wiki/Bay_of_ConcepciC3B3n.html</td>\n",
       "      <td>58</td>\n",
       "      <td>447</td>\n",
       "      <td>aps, aerial photos, and other data for this lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./wiki/Bay_of_ConcepciC3B3n.html</td>\n",
       "      <td>58</td>\n",
       "      <td>692</td>\n",
       "      <td>aps, aerial photos, and other data for this lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               File  Line  Index  \\\n",
       "0  ./wiki/Bay_of_ConcepciC3B3n.html     6    422   \n",
       "1  ./wiki/Bay_of_ConcepciC3B3n.html    45    628   \n",
       "2  ./wiki/Bay_of_ConcepciC3B3n.html    45    650   \n",
       "3  ./wiki/Bay_of_ConcepciC3B3n.html    58    447   \n",
       "4  ./wiki/Bay_of_ConcepciC3B3n.html    58    692   \n",
       "\n",
       "                                             Context  \n",
       "0  egories\":[\"Coordinates on Wikidata\",\"All stub ...  \n",
       "1  78-sj18-04-quiriquina.jpg 2x\" data-file-width=...  \n",
       "2  jpg 2x\" data-file-width=\"960\" data-file-height...  \n",
       "3  aps, aerial photos, and other data for this lo...  \n",
       "4  aps, aerial photos, and other data for this lo...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "rows = [[\"File\", \"Line\", \"Index\", \"Context\"]]\n",
    "\n",
    "for fn in word_locations:\n",
    "    with open(fn) as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "    context_step = 30\n",
    "\n",
    "    for line, iof in word_locations[fn]:\n",
    "        line_len = len(lines[line])\n",
    "        context_start = max(iof - context_step, 0)\n",
    "        context_end = min(iof + len(target_word) + context_step, line_len)\n",
    "        rows.append([fn, line, iof, lines[line][context_start:context_end + 1]])\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('result.csv', mode='w') as file:\n",
    "    csv.writer(file).writerows(rows)\n",
    "        \n",
    "        \n",
    "import pandas as pd\n",
    "df = pd.read_csv('result.csv')\n",
    "\n",
    "df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
